diff --git a/lavis/datasets/data_utils.py b/lavis/datasets/data_utils.py
index 098c7e0..1bbef17 100644
--- a/lavis/datasets/data_utils.py
+++ b/lavis/datasets/data_utils.py
@@ -13,21 +13,29 @@ import tarfile
 import zipfile
 import cv2
 
-import decord
+try:
+    import decord
+    from decord import VideoReader
+    decord.bridge.set_bridge("torch")
+except ImportError:
+    print("⚠️ Warning: 'decord' not found. Video loading will fail, but Image tasks will work.")
+    decord = None
+    VideoReader = None
 import webdataset as wds
 import numpy as np
 import torch
 from torch.utils.data.dataset import IterableDataset, ChainDataset
-from decord import VideoReader
 from lavis.common.registry import registry
 from lavis.datasets.datasets.base_dataset import ConcatDataset
 from tqdm import tqdm
-
-decord.bridge.set_bridge("torch")
 MAX_INT = registry.get("MAX_INT")
 
 
 def load_video(video_path, n_frms=MAX_INT, height=-1, width=-1, sampling="uniform"):
+    if VideoReader is None:
+        raise ImportError(
+            "decord is required for video loading. Install it with: conda install -c conda-forge decord"
+        )
     vr = VideoReader(uri=video_path, height=height, width=width)
 
     vlen = len(vr)
@@ -51,7 +59,7 @@ def load_video(video_path, n_frms=MAX_INT, height=-1, width=-1, sampling="unifor
 
 
 def apply_to_sample(f, sample):
-    ## add check for datasets that return none samples for missing items
+    # add check for datasets that return none samples for missing items
     if sample == None or len(sample) == 0:
         return {}
 
@@ -160,16 +168,19 @@ def concat_datasets(datasets):
             # if len(iterable_datasets) > 0:
             # concatenate map-style datasets and iterable-style datasets separately
             chained_datasets = (
-                ChainDataset(iterable_datasets) if len(iterable_datasets) > 0 else None
+                ChainDataset(iterable_datasets) if len(
+                    iterable_datasets) > 0 else None
             )
             concat_datasets = (
                 ConcatDataset(map_datasets) if len(map_datasets) > 0 else None
             )
 
             train_datasets = concat_datasets, chained_datasets
-            train_datasets = tuple([x for x in train_datasets if x is not None])
+            train_datasets = tuple(
+                [x for x in train_datasets if x is not None])
             train_datasets = (
-                train_datasets[0] if len(train_datasets) == 1 else train_datasets
+                train_datasets[0] if len(
+                    train_datasets) == 1 else train_datasets
             )
 
             datasets[split_name] = train_datasets
@@ -298,7 +309,8 @@ def uniform_frame_sampling(video_path, num_frames, target_height, target_width,
 
     start_frame = int(start_time * frame_rate)
     end_frame = int(end_time * frame_rate)
-    frame_indices = list(range(start_frame, end_frame + 1, (end_frame - start_frame + 1) // num_frames))
+    frame_indices = list(range(start_frame, end_frame + 1,
+                         (end_frame - start_frame + 1) // num_frames))
 
     frames = []
     for frame_index in frame_indices:
@@ -325,7 +337,8 @@ def head_tail_frame_sampling(video_path, num_frames, target_height, target_width
 
     start_frame = int(start_time * frame_rate)
     end_frame = int(end_time * frame_rate)
-    frame_indices = [start_frame] + [start_frame + (end_frame - start_frame) // (num_frames - 1) * i for i in range(1, num_frames - 1)] + [end_frame]
+    frame_indices = [start_frame] + [start_frame + (end_frame - start_frame) // (
+        num_frames - 1) * i for i in range(1, num_frames - 1)] + [end_frame]
 
     frames = []
     for frame_index in frame_indices:
@@ -339,7 +352,7 @@ def head_tail_frame_sampling(video_path, num_frames, target_height, target_width
     cap.release()
     if len(frames) == 0:
         return None
-    return torch.stack([torch.tensor(f).permute(2,0,1).float() for f in frames], dim=1)
+    return torch.stack([torch.tensor(f).permute(2, 0, 1).float() for f in frames], dim=1)
 
 
 def load_clip(video_path, num_frames, target_height, target_width, start_time=None, end_time=None, sampling="headtail"):
@@ -348,4 +361,4 @@ def load_clip(video_path, num_frames, target_height, target_width, start_time=No
     elif sampling == "uniform":
         return uniform_frame_sampling(video_path, num_frames, target_height, target_width, start_time, end_time)
     else:
-        raise NotImplementedError
\ No newline at end of file
+        raise NotImplementedError
diff --git a/lavis/models/blip_diffusion_models/modeling_ctx_clip.py b/lavis/models/blip_diffusion_models/modeling_ctx_clip.py
index 737b77d..f8a201b 100644
--- a/lavis/models/blip_diffusion_models/modeling_ctx_clip.py
+++ b/lavis/models/blip_diffusion_models/modeling_ctx_clip.py
@@ -13,10 +13,28 @@ from transformers.models.clip.configuration_clip import CLIPTextConfig
 from transformers.models.clip.modeling_clip import (
     CLIPEncoder,
     CLIPPreTrainedModel,
-    _expand_mask,
 )
 
 
+def _expand_mask(mask: torch.Tensor, dtype: torch.dtype) -> torch.Tensor:
+    """
+    Expand attention mask from [bsz, seq_len] to [bsz, 1, tgt_seq_len, src_seq_len].
+    Converts binary mask (1 for valid, 0 for padding) to attention mask format.
+    """
+    bsz, src_len = mask.size()
+    tgt_len = src_len
+
+    # Expand to 4D: [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
+    expanded_mask = mask[:, None, None, :].expand(
+        bsz, 1, tgt_len, src_len).to(dtype)
+
+    # Invert so that 0 (padding) becomes 1 and 1 (valid) becomes 0
+    inverted_mask = 1.0 - expanded_mask
+
+    # Fill padding positions with very negative values so they get masked out in softmax
+    return inverted_mask.masked_fill(inverted_mask.bool(), torch.finfo(dtype).min)
+
+
 class CtxCLIPTextModel(CLIPPreTrainedModel):
     config_class = CLIPTextConfig
 
@@ -190,7 +208,8 @@ class CtxCLIPTextEmbeddings(nn.Module):
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
-            "position_ids", torch.arange(config.max_position_embeddings).expand((1, -1))
+            "position_ids", torch.arange(
+                config.max_position_embeddings).expand((1, -1))
         )
 
     def forward(
diff --git a/lavis/models/ulip_models/utils/io.py b/lavis/models/ulip_models/utils/io.py
index d0edd1d..f277984 100755
--- a/lavis/models/ulip_models/utils/io.py
+++ b/lavis/models/ulip_models/utils/io.py
@@ -1,8 +1,13 @@
 import h5py
 import numpy as np
-import open3d
 import os
 
+try:
+    import open3d
+except ImportError:
+    open3d = None
+
+
 class IO:
     @classmethod
     def get(cls, file_path):
@@ -23,11 +28,16 @@ class IO:
     @classmethod
     def _read_npy(cls, file_path):
         return np.load(file_path)
-       
+
     # References: https://github.com/dimatura/pypcd/blob/master/pypcd/pypcd.py#L275
     # Support PCD files without compression ONLY!
     @classmethod
     def _read_pcd(cls, file_path):
+        if open3d is None:
+            raise ImportError(
+                "open3d is required to read .pcd (point cloud) files. "
+                "Install it with: pip install open3d"
+            )
         pc = open3d.io.read_point_cloud(file_path)
         ptcloud = np.array(pc.points)
         return ptcloud
@@ -39,4 +49,4 @@ class IO:
     @classmethod
     def _read_h5(cls, file_path):
         f = h5py.File(file_path, 'r')
-        return f['data'][()]
\ No newline at end of file
+        return f['data'][()]
diff --git a/requirements.txt b/requirements.txt
index 3f24699..8361a8a 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,5 +1,4 @@
 contexttimer
-decord
 diffusers<=0.16.0
 einops>=0.4.1
 fairscale==0.4.4
@@ -7,7 +6,7 @@ ftfy
 iopath
 ipython
 omegaconf
-opencv-python-headless==4.5.5.64
+opencv-python-headless
 opendatasets
 packaging
 pandas
@@ -24,7 +23,7 @@ timm==0.4.12
 torch>=1.10.0
 torchvision
 tqdm
-transformers==4.33.2
+transformers>=4.33.2
 webdataset
 wheel
 torchaudio
@@ -35,5 +34,4 @@ peft
 
 easydict==1.9
 pyyaml_env_tag==0.1
-open3d==0.13.0
 h5py
